{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfddd351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded 19,477 total protein-coding genes\n",
      "âœ… 19,357 sequences â‰¤ 3000 aa (GPU safe)\n",
      "âš ï¸ 120 sequences > 3000 aa (skipped)\n",
      "ðŸ’¾ Saved safe/long sequence metadata to ./data/ensembl/filtered\n",
      "\n",
      "ðŸ”§ Loading model: esm2_t6_8M_UR50D\n",
      "ðŸ§  Using 2 GPUs\n",
      "âœ… Model ready\n",
      "\n",
      "ðŸš€ Generating embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19357/19357 [04:58<00:00, 64.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Saved embeddings â†’ ./data/embeddings/esm2_t6_8M_UR50D_gene_embeddings.pt\n",
      "âœ… Shape: torch.Size([19357, 320]) [genes Ã— dim=320]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ESM2 Protein Embeddings (Filtered)\n",
    "\n",
    "Purpose:\n",
    "- Generate one fixed-size protein language model embedding per gene_symbol using ESM2.\n",
    "- Filter sequences by length to avoid OOM and keep GPU-safe runs.\n",
    "\n",
    "What this notebook does:\n",
    "- Loads curated protein_coding_genes.csv, cleans sequences, computes seq_len.\n",
    "- Splits into safe (â‰¤ MAX_SAFE_LEN aa) and too-long sets; writes both lists.\n",
    "- Runs ESM2 (MODEL_NAME, TARGET_LAYER) and mean-pools per-sequence token reps.\n",
    "- Saves:\n",
    "  - Torch file with {embeddings [genesÃ—dim], genes [order], model name}\n",
    "  - CSV gene order for downstream alignment.\n",
    "\n",
    "Reasons:\n",
    "- These embeddings are the gene-identity vectors consumed by the model.\n",
    "- Dimensionality and order must match training/inference configs exactly.\n",
    "- Reproducible, filtered set prevents memory issues and ensures consistency.\n",
    "\n",
    "Inputs:  ./data/ensembl/protein_coding_genes.csv\n",
    "Outputs: ./data/embeddings/{MODEL_NAME}_gene_embeddings.pt, gene_order.csv\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from esm import pretrained\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ============================================================\n",
    "# CONFIG\n",
    "# ============================================================\n",
    "PROTEIN_CSV = \"./data/ensembl/protein_coding_genes.csv\"\n",
    "MODEL_NAME = \"esm2_t6_8M_UR50D\"\n",
    "TARGET_LAYER = 6\n",
    "MAX_SAFE_LEN = 3000\n",
    "OUTDIR = \"./data/embeddings\"\n",
    "FILTER_DIR = \"./data/ensembl/filtered\"\n",
    "\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "os.makedirs(FILTER_DIR, exist_ok=True)\n",
    "\n",
    "BATCH_SIZE = 1  # conservative for GPU\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "torch.cuda.empty_cache()\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ============================================================\n",
    "# LOAD DATA\n",
    "# ============================================================\n",
    "df = pd.read_csv(PROTEIN_CSV)\n",
    "df[\"seq\"] = df[\"seq\"].astype(str).str.upper().str.replace(\"*\", \"\", regex=False)\n",
    "df[\"seq_len\"] = df[\"seq\"].str.len()\n",
    "\n",
    "safe_df = df[df[\"seq_len\"] <= MAX_SAFE_LEN].reset_index(drop=True)\n",
    "long_df = df[df[\"seq_len\"] > MAX_SAFE_LEN].reset_index(drop=True)\n",
    "\n",
    "print(f\"âœ… Loaded {len(df):,} total protein-coding genes\")\n",
    "print(f\"âœ… {len(safe_df):,} sequences â‰¤ {MAX_SAFE_LEN} aa (GPU safe)\")\n",
    "print(f\"âš ï¸ {len(long_df):,} sequences > {MAX_SAFE_LEN} aa (skipped)\")\n",
    "\n",
    "safe_path = os.path.join(FILTER_DIR, \"safe_sequences.csv\")\n",
    "long_path = os.path.join(FILTER_DIR, \"too_long_sequences.csv\")\n",
    "\n",
    "safe_df[[\"gene_symbol\", \"seq_len\"]].to_csv(safe_path, index=False)\n",
    "long_df[[\"gene_symbol\", \"seq_len\"]].to_csv(long_path, index=False)\n",
    "print(f\"ðŸ’¾ Saved safe/long sequence metadata to {FILTER_DIR}\")\n",
    "\n",
    "# ============================================================\n",
    "# LOAD MODEL\n",
    "# ============================================================\n",
    "print(f\"\\nðŸ”§ Loading model: {MODEL_NAME}\")\n",
    "model, alphabet = pretrained.__dict__[MODEL_NAME]()\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"ðŸ§  Using {torch.cuda.device_count()} GPUs\")\n",
    "    model = torch.nn.DataParallel(model)\n",
    "model = model.to(DEVICE).eval()\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "print(\"âœ… Model ready\")\n",
    "\n",
    "# ============================================================\n",
    "# DATASET + DATALOADER\n",
    "# ============================================================\n",
    "class ProteinDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.df = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        return row[\"seq\"], row[\"gene_symbol\"]\n",
    "\n",
    "def collate_batch(batch):\n",
    "    seqs, ids = zip(*batch)\n",
    "    batch_labels, batch_strs, batch_tokens = batch_converter(list(zip(ids, seqs)))\n",
    "    lengths = (batch_tokens != alphabet.padding_idx).sum(1)\n",
    "    return batch_tokens.to(DEVICE), lengths, ids\n",
    "\n",
    "dataset = ProteinDataset(safe_df)\n",
    "data_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)\n",
    "\n",
    "# ============================================================\n",
    "# GENERATE EMBEDDINGS\n",
    "# ============================================================\n",
    "embeddings, gene_order = [], []\n",
    "print(\"\\nðŸš€ Generating embeddings...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_tokens, batch_lens, ids in tqdm(data_loader, total=len(data_loader)):\n",
    "        # Forward with correct layer exposure under DP or single-GPU\n",
    "        if isinstance(model, torch.nn.DataParallel):\n",
    "            results = model.module(batch_tokens, repr_layers=[TARGET_LAYER], return_contacts=False)\n",
    "        else:\n",
    "            results = model(batch_tokens, repr_layers=[TARGET_LAYER], return_contacts=False)\n",
    "\n",
    "        reps = results[\"representations\"][TARGET_LAYER]\n",
    "\n",
    "        for i, tokens_len in enumerate(batch_lens):\n",
    "            # Mean-pool over residue tokens (exclude special tokens)\n",
    "            seq_rep = reps[i, 1:tokens_len - 1].mean(0).cpu()\n",
    "            embeddings.append(seq_rep)\n",
    "            gene_order.append(ids[i])\n",
    "\n",
    "# ============================================================\n",
    "# SAVE RESULTS\n",
    "# ============================================================\n",
    "E_ESM2 = torch.stack(embeddings)\n",
    "save_pt = os.path.join(OUTDIR, f\"{MODEL_NAME}_gene_embeddings.pt\")\n",
    "save_csv = os.path.join(OUTDIR, f\"{MODEL_NAME}_gene_order.csv\")\n",
    "\n",
    "torch.save({\"embeddings\": E_ESM2, \"genes\": gene_order, \"model\": MODEL_NAME}, save_pt)\n",
    "pd.Series(gene_order, name=\"gene_symbol\").to_csv(save_csv, index=False)\n",
    "\n",
    "print(f\"\\nâœ… Saved embeddings â†’ {save_pt}\")\n",
    "print(f\"âœ… Shape: {E_ESM2.shape} [genes Ã— dim={E_ESM2.shape[1]}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23513be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Loading model: esm2_t6_8M_UR50D\n",
      "ðŸ§  Using 2 GPUs\n",
      "âœ… Model ready\n",
      "âœ… Loaded 19,885 unique protein-coding genes\n",
      "ðŸš€ Generating embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19765/19765 [05:45<00:00, 57.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Saved embeddings â†’ ./data/embeddings/esm2_t6_8M_UR50D_gene_embeddings.pt\n",
      "âœ… Shape: torch.Size([19765, 320]) [genes Ã— dim=320]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "generate_esm2_embeddings.py\n",
    "Generates one ESM2 embedding per gene_symbol.\n",
    "No truncation or assumptions â€” uses ESM defaults.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from esm import pretrained\n",
    "from tqdm import tqdm\n",
    "from Bio import SeqIO\n",
    "\n",
    "# ============================================================\n",
    "# CONFIG\n",
    "# ============================================================\n",
    "FASTA_PATH = \"./data/ensembl/Homo_sapiens.GRCh38.pep.all.fa\"\n",
    "# MODEL_NAME = \"esm2_t33_650M_UR50D\"\n",
    "# MODEL_NAME = \"esm2_t12_35M_UR50D\"\n",
    "MODEL_NAME = \"esm2_t6_8M_UR50D\"\n",
    "target_layer = 6\n",
    "OUTDIR = \"./data/embeddings\"\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "\n",
    "BATCH_SIZE = 1  # adjust for GPU memory\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "torch.cuda.empty_cache()\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# LOAD MODEL\n",
    "# ============================================================\n",
    "print(f\"ðŸ”§ Loading model: {MODEL_NAME}\")\n",
    "model, alphabet = pretrained.__dict__[MODEL_NAME]()\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"ðŸ§  Using {torch.cuda.device_count()} GPUs\")\n",
    "    model = torch.nn.DataParallel(model)\n",
    "model = model.to(DEVICE).eval()\n",
    "print(\"âœ… Model ready\")\n",
    "\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "\n",
    "# ============================================================\n",
    "# PARSE FASTA\n",
    "# ============================================================\n",
    "records = []\n",
    "for rec in SeqIO.parse(FASTA_PATH, \"fasta\"):\n",
    "    parts = {k: v for k, v in (tok.split(\":\", 1) for tok in rec.description.split() if \":\" in tok)}\n",
    "    gene_symbol = parts.get(\"gene_symbol\")\n",
    "    if not gene_symbol or not gene_symbol.strip():\n",
    "        continue\n",
    "\n",
    "    seq = str(rec.seq).strip().upper().replace(\"*\", \"\")  # remove stop codons\n",
    "    records.append({\"gene_symbol\": gene_symbol.strip(), \"seq\": seq})\n",
    "\n",
    "df = pd.DataFrame(records).drop_duplicates(\"gene_symbol\").reset_index(drop=True)\n",
    "print(f\"âœ… Loaded {len(df):,} unique protein-coding genes\")\n",
    "\n",
    "# ============================================================\n",
    "# DATASET + DATALOADER\n",
    "# ============================================================\n",
    "class ProteinDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.df = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        return row[\"seq\"], row[\"gene_symbol\"]\n",
    "\n",
    "def collate_batch(batch):\n",
    "    seqs, ids = zip(*batch)\n",
    "    batch_labels, batch_strs, batch_tokens = batch_converter(list(zip(ids, seqs)))\n",
    "    lengths = (batch_tokens != alphabet.padding_idx).sum(1)\n",
    "    return batch_tokens.to(DEVICE), lengths, ids\n",
    "\n",
    "dataset = ProteinDataset(safe_df)\n",
    "data_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)\n",
    "\n",
    "# ============================================================\n",
    "# GENERATE EMBEDDINGS\n",
    "# ============================================================\n",
    "embeddings, gene_order = [], []\n",
    "print(\"ðŸš€ Generating embeddings...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_tokens, batch_lens, ids in tqdm(data_loader, total=len(data_loader)):\n",
    "        # results = model(batch_tokens, repr_layers=[33], return_contacts=False)\n",
    "        target_layer = 6  # for esm2_t12_35M_UR50D\n",
    "        results = model.module(batch_tokens, repr_layers=[target_layer], return_contacts=False)\n",
    "        reps = results[\"representations\"][target_layer]\n",
    "\n",
    "        # results = model.module(batch_tokens, repr_layers=[33], return_contacts=False)\n",
    "        # reps = results[\"representations\"][33]\n",
    "\n",
    "        for i, tokens_len in enumerate(batch_lens):\n",
    "            seq_rep = reps[i, 1:tokens_len - 1].mean(0).cpu()  # mean-pool actual length\n",
    "            embeddings.append(seq_rep)\n",
    "            gene_order.append(ids[i])\n",
    "\n",
    "# ============================================================\n",
    "# SAVE RESULTS\n",
    "# ============================================================\n",
    "E_ESM2 = torch.stack(embeddings)\n",
    "save_path = os.path.join(OUTDIR, f\"{MODEL_NAME}_gene_embeddings.pt\")\n",
    "\n",
    "torch.save(\n",
    "    {\"embeddings\": E_ESM2, \"genes\": gene_order, \"model\": MODEL_NAME},\n",
    "    save_path\n",
    ")\n",
    "\n",
    "pd.Series(gene_order, name=\"gene_symbol\").to_csv(\n",
    "    os.path.join(OUTDIR, f\"{MODEL_NAME}_gene_order.csv\"), index=False\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Saved embeddings â†’ {save_path}\")\n",
    "print(f\"âœ… Shape: {E_ESM2.shape} [genes Ã— dim={E_ESM2.shape[1]}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96177898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Safe sequences (short) ===\n",
      "   gene_symbol  seq_len\n",
      "0  IGHV1OR21-1      117\n",
      "1      IGKV5-2      115\n",
      "2      IGKV1-5      117\n",
      "3      IGKV1-6      117\n",
      "4      IGKV3-7      116 \n",
      "Total: 19765\n",
      "\n",
      "=== Too long sequences (>3000 aa) ===\n",
      "  gene_symbol  seq_len\n",
      "0      MUC5AC     5654\n",
      "1        MUC2     5130\n",
      "2       FCGBP     5405\n",
      "3        APOB     4563\n",
      "4       MUC16    15287 \n",
      "Total: 120\n",
      "\n",
      "=== Embeddings preview ===\n",
      "Tensor shape: torch.Size([19765, 320])  â†’  (19765 genes Ã— 320 dims)\n",
      "First 5 genes: ['IGHV1OR21-1', 'IGKV5-2', 'IGKV1-5', 'IGKV1-6', 'IGKV3-7']\n",
      "First embedding vector (first 10 dims): tensor([-0.2124, -0.1109,  0.0512, -0.0074,  0.3155, -0.1443,  0.0293, -0.0276,\n",
      "        -0.2905,  0.0407])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_80764/1784561354.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  emb = torch.load(\"./data/embeddings/esm2_t6_8M_UR50D_gene_embeddings.pt\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Preview filtered lists\n",
    "safe = pd.read_csv(\"./data/ensembl/filtered/safe_sequences.csv\")\n",
    "long = pd.read_csv(\"./data/ensembl/filtered/too_long_sequences.csv\")\n",
    "\n",
    "print(\"=== Safe sequences (short) ===\")\n",
    "print(safe.head(), \"\\nTotal:\", len(safe))\n",
    "\n",
    "print(\"\\n=== Too long sequences (>3000 aa) ===\")\n",
    "print(long.head(), \"\\nTotal:\", len(long))\n",
    "\n",
    "# Preview embeddings\n",
    "emb = torch.load(\"./data/embeddings/esm2_t6_8M_UR50D_gene_embeddings.pt\")\n",
    "E = emb[\"embeddings\"]\n",
    "genes = emb[\"genes\"]\n",
    "\n",
    "print(\"\\n=== Embeddings preview ===\")\n",
    "print(f\"Tensor shape: {E.shape}  â†’  ({E.shape[0]} genes Ã— {E.shape[1]} dims)\")\n",
    "print(\"First 5 genes:\", genes[:5])\n",
    "print(\"First embedding vector (first 10 dims):\", E[0][:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f97d179",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esm2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
