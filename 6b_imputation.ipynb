{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8ea1a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/walt/miniconda3/envs/esm2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "from performer_pytorch import Performer\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1️⃣ ROTARY EXPRESSION EMBEDDING\n",
    "# ============================================================\n",
    "class PositionalExprEmbedding(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.mask_token_id = -10  # same as BulkFormer\n",
    "        self.inv_freq = nn.Parameter(\n",
    "            1.0 / (100 ** (torch.arange(0, dim, 2).float() / dim)),\n",
    "            requires_grad=False\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x : [batch, genes]\n",
    "        mask_idx = (x == self.mask_token_id).nonzero()\n",
    "\n",
    "        rot = torch.einsum(\"bi,j->bij\", x, self.inv_freq)\n",
    "        rot = torch.cat((rot.sin(), rot.cos()), dim=-1)\n",
    "\n",
    "        # zero out masked tokens\n",
    "        if len(mask_idx) > 0:\n",
    "            rot[mask_idx[:, 0], mask_idx[:, 1]] = 0\n",
    "\n",
    "        return rot\n",
    "\n",
    "# ============================================================\n",
    "# 2️⃣ GBFormer (GAT + Performer blocks)\n",
    "# ============================================================\n",
    "class GBFormer(nn.Module):\n",
    "    def __init__(self, dim, gene_length,\n",
    "                 bin_head=2, full_head=2, bins=6, p_repeat=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "        self.bins = bins\n",
    "        self.gene_length = gene_length\n",
    "\n",
    "        # GAT module\n",
    "        self.g = GATv2Conv(dim, dim, add_self_loops=False)\n",
    "\n",
    "        # bin selector\n",
    "        self.which_b = nn.Linear(dim, 1)\n",
    "\n",
    "        # small Performer heads\n",
    "        self.b = nn.ModuleList([\n",
    "            Performer(\n",
    "                dim=dim,\n",
    "                heads=bin_head,\n",
    "                dim_head=32,\n",
    "                depth=1,\n",
    "                attn_dropout=0.1,\n",
    "                reversible=False\n",
    "            )\n",
    "            for _ in range(bins)\n",
    "        ])\n",
    "\n",
    "        self.f = nn.ModuleList([\n",
    "            Performer(\n",
    "                dim=dim,\n",
    "                heads=full_head,\n",
    "                dim_head=32,\n",
    "                depth=1,\n",
    "                attn_dropout=0.1,\n",
    "                reversible=False\n",
    "            )\n",
    "            for _ in range(p_repeat)\n",
    "        ])\n",
    "\n",
    "        self.ln = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # x: [B, G, E]\n",
    "        B, G, E = x.shape\n",
    "\n",
    "        x = self.ln(x)\n",
    "\n",
    "        # --- SAFE GRAPH UPDATE (no in-place) ---\n",
    "        x_graph = []\n",
    "        for b in range(B):\n",
    "            gx = self.g(x[b], edge_index)\n",
    "            x_graph.append(x[b] + gx)\n",
    "        x = torch.stack(x_graph, dim=0)\n",
    "\n",
    "        # --- choose bins ---\n",
    "        scores = self.which_b(x).squeeze(-1)      # [B, G]\n",
    "        order = torch.argsort(scores, dim=1, descending=True)  # [B, G]\n",
    "        order_exp = order.unsqueeze(-1).expand(-1, -1, E)\n",
    "\n",
    "        # reorder (no in-place)\n",
    "        x_sorted = torch.gather(x, 1, order_exp)\n",
    "\n",
    "        # split\n",
    "        n = (G - 1) // self.bins + 1\n",
    "        chunks = torch.split(x_sorted, n, dim=1)\n",
    "\n",
    "        # run performer per chunk\n",
    "        outs = []\n",
    "        for chunk, layer in zip(chunks, self.b):\n",
    "            outs.append(layer(chunk))\n",
    "        xs = torch.cat(outs, dim=1)\n",
    "\n",
    "        # --- UNSORT (no inplace scatter_) ---\n",
    "        out = torch.zeros_like(xs)\n",
    "        out = out.scatter(1, order_exp, xs)   # **THIS IS NOT INPLACE**\n",
    "\n",
    "        # --- global Performer ---\n",
    "        for layer in self.f:\n",
    "            out = layer(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3️⃣ BULKFORMER MODEL (Modified)\n",
    "# ============================================================\n",
    "class BulkFormer(nn.Module):\n",
    "    def __init__(self, dim, graph, gene_emb, gene_length,\n",
    "                 bin_head=2, full_head=2, bins=10, gb_repeat=2, p_repeat=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "        self.graph = graph\n",
    "        self.gene_length = gene_length\n",
    "\n",
    "        # 320-dim ESM2 embedding\n",
    "        self.gene_emb = nn.Parameter(gene_emb)\n",
    "\n",
    "        self.gene_emb_proj = nn.Sequential(\n",
    "            nn.Linear(gene_emb.shape[1], 4 * dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * dim, dim)\n",
    "        )\n",
    "\n",
    "        self.expr_emb = PositionalExprEmbedding(dim)\n",
    "\n",
    "        # AE latent → sample context vector\n",
    "        self.ae_enc = nn.Sequential(\n",
    "            nn.Linear(gene_length, 4 * dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * dim, dim)\n",
    "        )\n",
    "\n",
    "        self.x_proj = nn.Sequential(\n",
    "            nn.Linear(dim, 4 * dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * dim, dim)\n",
    "        )\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            GBFormer(dim, gene_length,\n",
    "                     bin_head=bin_head,\n",
    "                     full_head=full_head,\n",
    "                     bins=bins,\n",
    "                     p_repeat=p_repeat)\n",
    "            for _ in range(gb_repeat)\n",
    "        ])\n",
    "\n",
    "        self.ln = nn.LayerNorm(dim)\n",
    "\n",
    "        # Final head predicts expression\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(dim, 4 * dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, ae_latent=None):\n",
    "        # x: [B, G]\n",
    "        B, G = x.shape\n",
    "\n",
    "        gene_tok = self.gene_emb_proj(self.gene_emb).unsqueeze(0)  # [1, G, dim]\n",
    "        expr_tok = self.expr_emb(x)                                # [B, G, dim]\n",
    "        ae_tok = ae_latent.unsqueeze(1)                            # [B, 1, dim]\n",
    "\n",
    "        x = expr_tok + gene_tok + ae_tok\n",
    "        x = self.x_proj(x)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x, self.graph)\n",
    "\n",
    "        x = self.ln(x)\n",
    "\n",
    "        out = self.head(x).squeeze(-1)  # predict masked expression\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93fde210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation expression: torch.Size([9557, 19357])\n",
      "ESM2: torch.Size([19357, 320])\n"
     ]
    }
   ],
   "source": [
    "# Load expression validation set\n",
    "val_expr = pd.read_parquet(\n",
    "    \"./data/archs4/processed_short_proteins/val_expr_logtpm_short.parquet\"\n",
    ")\n",
    "\n",
    "X_val = torch.tensor(val_expr.T.values.astype(\"float32\"), device=device)\n",
    "N_val, G = X_val.shape\n",
    "\n",
    "print(\"Validation expression:\", X_val.shape)\n",
    "\n",
    "# Load ESM2 gene identity embeddings\n",
    "esm2_raw = torch.load(\"./data/embeddings/esm2_t6_8M_UR50D_gene_embeddings.pt\")\n",
    "esm2 = esm2_raw[\"embeddings\"].float().to(device)\n",
    "print(\"ESM2:\", esm2.shape)\n",
    "\n",
    "# # Load AE sample latent embeddings (if needed)\n",
    "# ae_val = torch.load(\"./data/embeddings/ae_gene_latents_320_val_set.pt\")\n",
    "# ae_val = torch.tensor(ae_val, dtype=torch.float32).to(device)\n",
    "# print(\"AE latent:\", ae_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef625ac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BulkFormer(\n",
       "  (gene_emb_proj): Sequential(\n",
       "    (0): Linear(in_features=320, out_features=1280, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "  )\n",
       "  (expr_emb): PositionalExprEmbedding()\n",
       "  (ae_enc): Sequential(\n",
       "    (0): Linear(in_features=19357, out_features=1280, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "  )\n",
       "  (x_proj): Sequential(\n",
       "    (0): Linear(in_features=320, out_features=1280, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "  )\n",
       "  (blocks): ModuleList(\n",
       "    (0): GBFormer(\n",
       "      (g): GATv2Conv(320, 320, heads=1)\n",
       "      (which_b): Linear(in_features=320, out_features=1, bias=True)\n",
       "      (b): ModuleList(\n",
       "        (0): Performer(\n",
       "          (net): SequentialSequence(\n",
       "            (layers): ModuleList(\n",
       "              (0): ModuleList(\n",
       "                (0): PreLayerNorm(\n",
       "                  (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): SelfAttention(\n",
       "                    (fast_attention): FastAttention(\n",
       "                      (kernel_fn): ReLU()\n",
       "                    )\n",
       "                    (to_q): Linear(in_features=320, out_features=64, bias=True)\n",
       "                    (to_k): Linear(in_features=320, out_features=64, bias=True)\n",
       "                    (to_v): Linear(in_features=320, out_features=64, bias=True)\n",
       "                    (to_out): Linear(in_features=64, out_features=320, bias=True)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (1): PreLayerNorm(\n",
       "                  (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Chunk(\n",
       "                    (fn): FeedForward(\n",
       "                      (w1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "                      (act): GELU(approximate='none')\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (w2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (proj_updater): ProjectionUpdater(\n",
       "            (instance): SequentialSequence(\n",
       "              (layers): ModuleList(\n",
       "                (0): ModuleList(\n",
       "                  (0): PreLayerNorm(\n",
       "                    (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): SelfAttention(\n",
       "                      (fast_attention): FastAttention(\n",
       "                        (kernel_fn): ReLU()\n",
       "                      )\n",
       "                      (to_q): Linear(in_features=320, out_features=64, bias=True)\n",
       "                      (to_k): Linear(in_features=320, out_features=64, bias=True)\n",
       "                      (to_v): Linear(in_features=320, out_features=64, bias=True)\n",
       "                      (to_out): Linear(in_features=64, out_features=320, bias=True)\n",
       "                      (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (1): PreLayerNorm(\n",
       "                    (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): Chunk(\n",
       "                      (fn): FeedForward(\n",
       "                        (w1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "                        (act): GELU(approximate='none')\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (w2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (f): ModuleList(\n",
       "        (0): Performer(\n",
       "          (net): SequentialSequence(\n",
       "            (layers): ModuleList(\n",
       "              (0): ModuleList(\n",
       "                (0): PreLayerNorm(\n",
       "                  (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): SelfAttention(\n",
       "                    (fast_attention): FastAttention(\n",
       "                      (kernel_fn): ReLU()\n",
       "                    )\n",
       "                    (to_q): Linear(in_features=320, out_features=64, bias=True)\n",
       "                    (to_k): Linear(in_features=320, out_features=64, bias=True)\n",
       "                    (to_v): Linear(in_features=320, out_features=64, bias=True)\n",
       "                    (to_out): Linear(in_features=64, out_features=320, bias=True)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (1): PreLayerNorm(\n",
       "                  (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Chunk(\n",
       "                    (fn): FeedForward(\n",
       "                      (w1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "                      (act): GELU(approximate='none')\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (w2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (proj_updater): ProjectionUpdater(\n",
       "            (instance): SequentialSequence(\n",
       "              (layers): ModuleList(\n",
       "                (0): ModuleList(\n",
       "                  (0): PreLayerNorm(\n",
       "                    (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): SelfAttention(\n",
       "                      (fast_attention): FastAttention(\n",
       "                        (kernel_fn): ReLU()\n",
       "                      )\n",
       "                      (to_q): Linear(in_features=320, out_features=64, bias=True)\n",
       "                      (to_k): Linear(in_features=320, out_features=64, bias=True)\n",
       "                      (to_v): Linear(in_features=320, out_features=64, bias=True)\n",
       "                      (to_out): Linear(in_features=64, out_features=320, bias=True)\n",
       "                      (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (1): PreLayerNorm(\n",
       "                    (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): Chunk(\n",
       "                      (fn): FeedForward(\n",
       "                        (w1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "                        (act): GELU(approximate='none')\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (w2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (ln): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "  (head): Sequential(\n",
       "    (0): Linear(in_features=320, out_features=1280, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=1280, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# Load model (assumes BulkFormer class already imported)\n",
    "# -------------------------------------------------------------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = BulkFormer(\n",
    "    dim=320,\n",
    "    graph=torch.load(\"./graph/edge_index_top20.pt\").long().to(device),\n",
    "    gene_emb=esm2,          # <-- MUST MATCH the same tensor shape\n",
    "    gene_length=G,\n",
    "    gb_repeat=1,\n",
    "    bins=1,\n",
    "    bin_head=2,\n",
    "    full_head=2,\n",
    "    p_repeat=1\n",
    ").to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(\"bulkformer_gbformer.pt\", map_location=device))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19189ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9446, 19357])\n"
     ]
    }
   ],
   "source": [
    "expr = pd.read_parquet(\n",
    "    \"./data/archs4/processed_short_proteins/test_expr_logtpm_short.parquet\"\n",
    ").T.values.astype(\"float32\")\n",
    "\n",
    "X = torch.tensor(expr).to(device)    # shape: [samples, genes]\n",
    "N, G = X.shape\n",
    "print(X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2899be5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_mask(x, mask_ratio=0.15):\n",
    "    mask = (torch.rand_like(x) < mask_ratio)\n",
    "    x_masked = x.clone()\n",
    "    x_masked[mask] = -10       # BulkFormer mask token\n",
    "    return x_masked, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30689ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_bulkformer(model, X, mask_ratio=0.15):\n",
    "    X_masked, mask = apply_mask(X, mask_ratio)\n",
    "    with torch.no_grad():\n",
    "        # AE latents = zeros if you are not using AE for inference\n",
    "        ae_latent = torch.zeros((X.shape[0], 320), device=device)\n",
    "        pred = model(X_masked, ae_latent)\n",
    "\n",
    "    # extract masked positions only\n",
    "    true_vals = X[mask].detach().cpu().numpy()\n",
    "    pred_vals = pred[mask].detach().cpu().numpy()\n",
    "\n",
    "    return pearsonr(true_vals, pred_vals)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1ec0a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_means = X.mean(dim=0, keepdim=True)\n",
    "\n",
    "def impute_mean(X, mask_ratio=0.15):\n",
    "    _, mask = apply_mask(X, mask_ratio)\n",
    "    true_vals = X[mask].cpu().numpy()\n",
    "    pred_vals = gene_means.repeat(X.shape[0], 1)[mask].cpu().numpy()\n",
    "    return pearsonr(true_vals, pred_vals)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1c58210",
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_medians = X.median(dim=0, keepdim=True).values\n",
    "\n",
    "def impute_median(X, mask_ratio=0.15):\n",
    "    _, mask = apply_mask(X, mask_ratio)\n",
    "    true_vals = X[mask].cpu().numpy()\n",
    "    pred_vals = gene_medians.repeat(X.shape[0], 1)[mask].cpu().numpy()\n",
    "    return pearsonr(true_vals, pred_vals)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3385bbbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFERENCE] Running GPU-optimized BulkFormer imputation...\n",
      "  [GPU] Creating masks... (shape: torch.Size([9446, 19357]))\n",
      "  [GPU] Processing 9446 samples in batches of 64...\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 5.91 GiB. GPU 0 has a total capacity of 24.00 GiB of which 0 bytes is free. Process 66668 has 17179869184.00 GiB memory in use. Of the allocated memory 18.40 GiB is allocated by PyTorch, and 4.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 70\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[INFERENCE] Running GPU-optimized BulkFormer imputation...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     69\u001b[0m results \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 70\u001b[0m results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBulkFormer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mimpute_bulkformer_batched_gpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMean\u001b[39m\u001b[38;5;124m\"\u001b[39m]       \u001b[38;5;241m=\u001b[39m impute_mean(X, \u001b[38;5;241m0.15\u001b[39m)\n\u001b[1;32m     72\u001b[0m results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMedian\u001b[39m\u001b[38;5;124m\"\u001b[39m]     \u001b[38;5;241m=\u001b[39m impute_median(X, \u001b[38;5;241m0.15\u001b[39m)\n",
      "Cell \u001b[0;32mIn[16], line 39\u001b[0m, in \u001b[0;36mimpute_bulkformer_batched_gpu\u001b[0;34m(model, X, mask_ratio, batch_size)\u001b[0m\n\u001b[1;32m     36\u001b[0m aeb \u001b[38;5;241m=\u001b[39m ae_latent[i:end_idx]\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Forward pass on GPU\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m pred_b \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maeb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m preds[i:end_idx] \u001b[38;5;241m=\u001b[39m pred_b\n\u001b[1;32m     42\u001b[0m batch_elapsed \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m batch_time\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[4], line 191\u001b[0m, in \u001b[0;36mBulkFormer.forward\u001b[0;34m(self, x, ae_latent)\u001b[0m\n\u001b[1;32m    188\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_proj(x)\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m--> 191\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    193\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln(x)\n\u001b[1;32m    195\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead(x)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# predict masked expression\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[4], line 109\u001b[0m, in \u001b[0;36mGBFormer.forward\u001b[0;34m(self, x, edge_index)\u001b[0m\n\u001b[1;32m    107\u001b[0m outs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(chunks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb):\n\u001b[0;32m--> 109\u001b[0m     outs\u001b[38;5;241m.\u001b[39mappend(\u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    110\u001b[0m xs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(outs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# --- UNSORT (no inplace scatter_) ---\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/esm2/lib/python3.10/site-packages/performer_pytorch/performer_pytorch.py:579\u001b[0m, in \u001b[0;36mPerformer.forward\u001b[0;34m(self, x, **kwargs)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_check_redraw:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj_updater\u001b[38;5;241m.\u001b[39mredraw_projections()\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/esm2/lib/python3.10/site-packages/performer_pytorch/reversible.py:139\u001b[0m, in \u001b[0;36mSequentialSequence.forward\u001b[0;34m(self, x, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (f, g), (f_args, g_args) \u001b[38;5;129;01min\u001b[39;00m layers_and_args:\n\u001b[1;32m    138\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m f(x, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mf_args)\n\u001b[0;32m--> 139\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[43mg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mg_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/esm2/lib/python3.10/site-packages/performer_pytorch/performer_pytorch.py:338\u001b[0m, in \u001b[0;36mPreLayerNorm.forward\u001b[0;34m(self, x, **kwargs)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 338\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/esm2/lib/python3.10/site-packages/performer_pytorch/performer_pytorch.py:349\u001b[0m, in \u001b[0;36mChunk.forward\u001b[0;34m(self, x, **kwargs)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunks \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 349\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    350\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunks, dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim)\n\u001b[1;32m    351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn(c, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m chunks], dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/esm2/lib/python3.10/site-packages/performer_pytorch/performer_pytorch.py:366\u001b[0m, in \u001b[0;36mFeedForward.forward\u001b[0;34m(self, x, **kwargs)\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    365\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglu:\n\u001b[0;32m--> 366\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mw1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    367\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(x)\n\u001b[1;32m    368\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:134\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    131\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m    Runs the forward pass.\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 5.91 GiB. GPU 0 has a total capacity of 24.00 GiB of which 0 bytes is free. Process 66668 has 17179869184.00 GiB memory in use. Of the allocated memory 18.40 GiB is allocated by PyTorch, and 4.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def impute_bulkformer_batched_gpu(model, X, mask_ratio=0.15, batch_size=64):\n",
    "    \"\"\"\n",
    "    Fast GPU-optimized batched imputation.\n",
    "    - Larger batch_size (64) to maximize GPU utilization\n",
    "    - Vectorized mask operations on GPU\n",
    "    - Minimal CPU transfers\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    batch_start = time.time()\n",
    "    N, G = X.shape\n",
    "    \n",
    "    # Create mask and masked input on GPU\n",
    "    print(f\"  [GPU] Creating masks... (shape: {X.shape})\")\n",
    "    mask = (torch.rand_like(X) < mask_ratio)\n",
    "    X_masked = X.clone()\n",
    "    X_masked[mask] = -10\n",
    "    \n",
    "    # Pre-allocate output on GPU\n",
    "    ae_latent = torch.zeros((N, 320), device=device)\n",
    "    preds = torch.zeros_like(X, device=device)\n",
    "    \n",
    "    print(f\"  [GPU] Processing {N} samples in batches of {batch_size}...\")\n",
    "    num_batches = (N + batch_size - 1) // batch_size\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, i in enumerate(range(0, N, batch_size)):\n",
    "            batch_time = time.time()\n",
    "            end_idx = min(i + batch_size, N)\n",
    "            batch_size_actual = end_idx - i\n",
    "            \n",
    "            xb = X_masked[i:end_idx]\n",
    "            aeb = ae_latent[i:end_idx]\n",
    "            \n",
    "            # Forward pass on GPU\n",
    "            pred_b = model(xb, aeb)\n",
    "            preds[i:end_idx] = pred_b\n",
    "            \n",
    "            batch_elapsed = time.time() - batch_time\n",
    "            throughput = batch_size_actual / batch_elapsed\n",
    "            print(f\"    Batch {batch_idx+1}/{num_batches} | \"\n",
    "                  f\"Samples {i+1}-{end_idx} | Time: {batch_elapsed:.2f}s | \"\n",
    "                  f\"Throughput: {throughput:.0f} samples/s\")\n",
    "    \n",
    "    # Extract masked positions only (keep on GPU until final computation)\n",
    "    true_vals = X[mask]\n",
    "    pred_vals = preds[mask]\n",
    "    \n",
    "    # Compute correlation on GPU for speed\n",
    "    pcc = np.corrcoef(true_vals.detach().cpu().numpy(), \n",
    "                      pred_vals.detach().cpu().numpy())[0, 1]\n",
    "    \n",
    "    total_time = time.time() - batch_start\n",
    "    print(f\"  ✓ Total inference time: {total_time:.2f}s ({N/total_time:.0f} samples/s)\")\n",
    "    \n",
    "    return {\n",
    "        \"true\": true_vals.detach().cpu().numpy(),\n",
    "        \"pred\": pred_vals.detach().cpu().numpy(),\n",
    "        \"pcc\": pcc,\n",
    "        \"time\": total_time\n",
    "    }\n",
    "\n",
    "\n",
    "# Run optimized GPU inference\n",
    "print(\"\\n[INFERENCE] Running GPU-optimized BulkFormer imputation...\")\n",
    "results = {}\n",
    "results[\"BulkFormer\"] = impute_bulkformer_batched_gpu(model, X, mask_ratio=0.15, batch_size=64)\n",
    "results[\"Mean\"]       = impute_mean(X, 0.15)\n",
    "results[\"Median\"]     = impute_median(X, 0.15)\n",
    "\n",
    "print(\"\\n[RESULTS] Imputation Performance (15% masked):\")\n",
    "for method, res in results.items():\n",
    "    if isinstance(res, dict):\n",
    "        print(f\"  {method:12} | PCC: {res['pcc']:.4f} | Time: {res.get('time', 'N/A')}\")\n",
    "    else:\n",
    "        print(f\"  {method:12} | PCC: {res:.4f}\")\n",
    "\n",
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fc62c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "plt.bar(results.keys(), results.values(), color=[\"steelblue\",\"gray\",\"darkgray\"])\n",
    "plt.ylabel(\"Pearson r\")\n",
    "plt.title(\"Imputation performance (15% missing)\")\n",
    "plt.ylim(0,1)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef8777c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[BENCHMARK] Testing masking ratios with GPU acceleration...\\n\")\n",
    "\n",
    "mask_rates = [0.05, 0.15, 0.25, 0.35]\n",
    "pcc_curve = []\n",
    "times = []\n",
    "\n",
    "for r in mask_rates:\n",
    "    print(f\"Testing mask ratio: {int(r*100)}%\")\n",
    "    start = time.time()\n",
    "    result = impute_bulkformer_batched_gpu(model, X, mask_ratio=r, batch_size=64)\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    pcc_curve.append(result[\"pcc\"])\n",
    "    times.append(elapsed)\n",
    "    print(f\"  → PCC: {result['pcc']:.4f} | Total time: {elapsed:.2f}s\\n\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n[BENCHMARK SUMMARY]\")\n",
    "print(\"Mask % | PCC    | Time (s) | Throughput\")\n",
    "print(\"-------|--------|----------|----------\")\n",
    "for r, p, t in zip(mask_rates, pcc_curve, times):\n",
    "    throughput = len(X) / t\n",
    "    print(f\"{int(r*100):5}% | {p:.4f} | {t:8.2f} | {throughput:8.0f} samples/s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab2eba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "plt.plot([r*100 for r in mask_rates], pcc_curve, marker=\"o\")\n",
    "plt.xlabel(\"Masking ratio (%)\")\n",
    "plt.ylabel(\"Pearson r\")\n",
    "plt.title(\"BulkFormer imputation vs missingness\")\n",
    "plt.ylim(0,1)\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ba1d29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esm2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
